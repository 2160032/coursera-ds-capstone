---
title: "Data Science Capstone Project"
author: patrick charles
date: "`r Sys.Date()`"
output:  
    html_document:
        keep_md: true
---

## Text Prediction (Exploratory Analysis and Prediction)

## Summary

A model and algorithms for text prediction are constructed. This is the final capstone project for the Johns Hopkins data science specialization certification series. 

In this dynamic document, the body of sample texts is loaded, exploratory analysis performed and a model and algorithm for word prediction built.

The [Capstone Dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) sample texts include content captured from blogs, new sources and twitter.

```{r global_options, include=FALSE}
  knitr::opts_chunk$set(fig.width=12, fig.height=8)

  local({
    r <- getOption("repos")
    r["CRAN"] <- "http://cran.cnr.berkeley.edu/"
    options(repos = r)
   })
```

```{r prereqs, message=FALSE, warning=FALSE, echo=FALSE}
## Prerequisites
  if(!require(tm)) install.packages("tm", dep=T)
  library(tm)
  if(!require(SnowballC)) install.packages("SnowballC", dep=T)
  library(SnowballC)
  if(!require(Rgraphviz)) {
    source("http://bioconductor.org/biocLite.R")
    biocLite("Rgraphviz")
  }
  if(!require(qdap)) install.packages("qdap", dep=T)
  library(qdap)
  if(!require(RWeka)) install.packages("RWeka", dep=T)
  library(RWeka)
  
  library(Rgraphviz)
  library(ggplot2)
  library(wordcloud)
  library(dplyr)
  library(slam)

  # custom functions for text prediction
  source("textPrediction.R")
```

## Load and Examine the Sample Texts

### Documents

The English-language content is used for the analysis.

```{r corpus.find}
  # view the English sample text source documents
  cpath <- file.path(".", "data", "final", "en_US")
  csize <- length(dir(cpath))
  dir(cpath)
```

```{r corpus.wc, echo=FALSE}
  wc <- function(ctree, corpus) {
    unlist(strsplit(sub("^ +", "",
      system(paste("wc ", ctree, corpus, sep=""),
      intern=TRUE)), split=" +"))
  }

  ctree <- "data/final/en_US/en_US."
  wc.blogs <- wc(ctree, "blogs.txt")
  wc.news <- wc(ctree, "news.txt")
  wc.twitter <- wc(ctree, "twitter.txt")

  wc.total.m <- round((as.numeric(wc.blogs[2]) +
                       as.numeric(wc.news[2]) +
                       as.numeric(wc.twitter[2]))/1000/1000, digits=0)

```
There are `r csize` documents in the English text samples.

* __blogs__ contains `r wc.blogs[1]` lines, `r wc.blogs[2]` words, and `r wc.blogs[3]` characters.
* __twitter__ contains `r wc.twitter[1]` lines, `r wc.twitter[2]` words, and `r wc.twitter[3]` characters.
* __news__ contains `r wc.news[1]` lines, `r wc.news[2]` words, and `r wc.news[3]` characters.

### Load Full Corpus of Texts

```{r corpus.load, cache=TRUE, echo=FALSE}

  blogs <- readLines("data/final/en_US/en_US.blogs.txt", skipNul=TRUE)
  twitter <- readLines("data/final/en_US/en_US.twitter.txt", skipNul=TRUE)
  news <- readLines("data/final/en_US/en_US.news.txt", skipNul=TRUE)

  texts.full <- c(blogs, news, twitter)

  name <- c("blog", "twitter", "news", "all")
  bytes <- c(object.size(blogs), object.size(twitter), object.size(news), object.size(texts.full))
  lines <- c(length(blogs), length(twitter), length(news), length(texts.full))
  corpus.info <- data.frame(name, bytes, lines)

  rm(blogs, twitter, news) # remove separate stores for each type
```

### Subset into a Training and Testing Corpus

```{r corpus.sub.load, cache=FALSE, echo=FALSE}
  texts.training <- sample(texts.full, 10000, replace=FALSE)
#  texts.training <- sample(texts.full, 500000, replace=FALSE)
  texts.testing <- sample(texts.full, 100, replace=FALSE)
```

## Cleaning and Transformation
```{r corpus.clean}
  profanity <- as.character(read.csv("data/profanity.txt", header=FALSE)$V1)
  # for unigrams, remove punctuation
  filtered.sub.np <- createCleanCorpus(texts.training,
    remove.punct=TRUE, remove.profanity=TRUE, profanity)
  # for generating predictive corpus, leave punctuation. tm/dtm uses.
  filtered.sub <- createCleanCorpus(texts.training,
    remove.punct=FALSE, remove.profanity=TRUE, profanity)
  # for generating test text, remove punctuation
  filtered.test <- createCleanCorpus(texts.testing, remove.punct=TRUE)

  filtered.sub
  filtered.test
```  


## Exploratory Analysis

A document-term matrix is created from the samples for the purpose of
analyzing word frequencies and characteristics.

### Most Frequently Occurring Terms

```{r explore.terms.freq}
  fthreshold <- 20 # frequency list entry threshold
  # minfreq <- 3 # too large. produces dim: 500000x229481 > 4503599627370496
  minfreq <- 10 # minimum required doc frequency for dtm
  dtm.1 <- DocumentTermMatrix(filtered.sub.np, control=list(minDocFreq=minfreq))
  freq.1 <- sort(colSums(as.matrix(dtm.1)), decreasing=TRUE)
  nf.1 <- data.frame(word=names(freq.1), freq=freq.1)
  
  findFreqTerms(dtm.1, lowfreq=nf.1$freq[fthreshold])
 
  # plot frequencies
  ggplot(subset(nf.1, freq>nf.1$freq[fthreshold]),
    aes(reorder(word, freq), freq)) +
    geom_bar(stat="identity") + 
    theme(axis.text.x=element_text(angle=45, hjust=1)) +
    ggtitle("Most Common Words") + xlab("Word") + ylab("Frequency") 
```

### Least Frequently Occurring Terms

```{r explore.terms.least}
  head(findFreqTerms(dtm.1, highfreq=1), 10)
```

### Wordcloud

The wordcloud is a graphical visualization of word occurrence where
size is scaled by frequency.

```{r explore.terms.wc}
  set.seed(482)
  wordcloud(names(freq.1), freq.1, min.freq=40, max.words=100,
    colors=brewer.pal(8, "Dark2"), rot.per=0.35, scale=c(5, 0.5))
```

### Word Length Frequency

A histogram of number of letters by word frequency illustrates
the distribution of word lengths and highlights the average word length.

```{r explore.terms.wfreq, echo=FALSE}
  words <- dtm.1 %>%
    as.matrix %>%
    colnames %>%
    (function(x) x[nchar(x) < 20])
  words.lengths <- data.frame(length=nchar(words))
  ggplot(words.lengths, aes(x=length)) +
    geom_histogram(binwidth=1) + 
    geom_vline(xintercept=mean(nchar(words)),
              color="red", size=1, alpha=0.5) + 
    labs(x="Letters", y="Words")
```

The average length word in the sample texts has `r round(mean(nchar(words)), digits=0)` characters.


### N-grams

n-grams are extracted to characterize the frequency of multi-word
clusters.

```{r explore.ngrams, cache=TRUE}
  # sentence delimiters; prevent clustering across sentence boundaries
  delimiters <- " \\t\\r\\n.!?,;\"()"

  # n-gram tokenizers
  BigramTokenizer <-
    function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
  TrigramTokenizer <-
    function(x, n) NGramTokenizer(x, Weka_control(min=3, max=3))
  QuadgramTokenizer <-
    function(x, n) NGramTokenizer(x, Weka_control(min=4, max=4))
  PentagramTokenizer <-
    function(x, n) NGramTokenizer(x, Weka_control(min=5, max=5))
 
  gthreshold <- 15 # threshold for number of gram matches to display
  options(mc.cores=1) # limit cores to prevent rweka processing problems

  ft.2 <- 3
  dtm.2 <- DocumentTermMatrix(filtered.sub, control=list(tokenize=BigramTokenizer, bounds=list(global=c(ft.2, Inf))))
  freq.2 <- sort(col_sums(dtm.2, na.rm=T), decreasing=TRUE)
  nf.2 <- data.frame(word=names(freq.2), freq=freq.2)
  plotGram(gthreshold, freq.2, nf.2, "Bigram")
  
  ft.3 <- 3
  dtm.3 <- DocumentTermMatrix(filtered.sub, control=list(tokenize=TrigramTokenizer, bounds=list(global=c(ft.3, Inf))))
  freq.3 <- sort(col_sums(dtm.3, na.rm=T), decreasing=TRUE)
  nf.3 <- data.frame(word=names(freq.3), freq=freq.3)
  plotGram(gthreshold, freq.3, nf.3, "Quadgram")
  
  ft.4 <- 2 
  dtm.4 <- DocumentTermMatrix(filtered.sub, control=list(tokenize=QuadgramTokenizer, bounds=list(global=c(ft.4, Inf))))
  freq.4 <- sort(col_sums(dtm.4, na.rm=T), decreasing=TRUE)
  nf.4 <- data.frame(word=names(freq.4), freq=freq.4)
  plotGram(gthreshold, freq.4, nf.4, "Quadgram")
  
  ft.5 <- 2
  dtm.5 <- DocumentTermMatrix(filtered.sub, control=list(tokenize=PentagramTokenizer, bounds=list(global=c(ft.5, Inf))))
  freq.5 <- sort(col_sums(dtm.5, na.rm=T), decreasing=TRUE)
  nf.5 <- data.frame(word=names(freq.5), freq=freq.5)
  plotGram(gthreshold, freq.5, nf.5, "Pentagram")
```

```{r load.ngrams, eval=TRUE}
#  r <- 10 # frequency span for last-resort randomization
#  nf <- list("f1"=nf.1, "f2"=nf.2, "f3"=nf.3, "f4"=nf.4, "f5"=nf.5, "r"=r)
#  save(nf, file="data/nFreq.Rda") # save the ngram frequencies to disk
  load("nFreq-200000-10-3-3-2-2.Rda")
```

### N-Gram Distribution

```{r optimization.count}
  # return the number of entries with frequency exceeding count
  countAboveFrequency <- function(nf, count) {
    dim(nf[nf$freq > count, ])[1]
  }
```

#### Total Count (Unique)
  * pentagrams: **`r countAboveFrequency(nf.5, 0)`**
  * quadgrams: **`r countAboveFrequency(nf.4, 0)`**
  * trigrams: **`r countAboveFrequency(nf.3, 0)`**
  * bigrams: **`r countAboveFrequency(nf.2, 0)`**
  * words: **`r countAboveFrequency(nf.1, 0)`**


## Prediction

### Prediction Tests (Unit Tests)

```{r predict.test}

  # 4-gram matches
  predictNextWord("could be a", nf)
  predictNextWord("i have to say thanks for the", nf)
  predictNextWord("a few years", nf)
  predictNextWord("the first time", nf)
  predictNextWord("i am so", nf)
  predictNextWord("ejefiei i am so", nf)
  
  # 3-gram matches
  predictNextWord("be a", nf)
  predictNextWord("can not", nf)
  predictNextWord("no matter", nf)
  predictNextWord("jefjieie no matter", nf)

  # 2-gram matches
  predictNextWord("a", nf)
  predictNextWord("will", nf)
  predictNextWord("could", nf)
  predictNextWord("ejfejke could", nf)

  # non-matches
  predictNextWord("jkefjiee", nf)
```

### Accuracy Tests

Random substrings are extracted from the testing text set.
The last word is excluded and the prediction model called on
the string. The actual last word is then compared with the predicted
last word, to guage the accuracy of the model.

```{r predict.accuracy}
  test.result <- testTimeAccuracy(filtered.test, nf)
```

**`r length(filtered.test)`** strings were set aside in a test dataset.

Substrings were randomly selected from the test data and the model 
used to predict the last word of the substring.

The measured accuracy of the model is **`r round(test.result$accuracy * 100, digits=2)`%**.

The average speed of the algorithm is **`r round(test.result$time[1] * 1000, digits=2)`ms** per word prediction.


## Optimizations 

### Test Results - Accuracy, Response Time and Dataset Size

* 10K texts, quadgrams, all punct stripped
** 10.1%, 8ms - load("nFreq-10000-1-1-1-1.Rda") 9.4B
* 10K texts, introduced pentagrams
** 8.4%, 285ms - load("nFreq-10000-1-1-1-1-1.Rda") 14.1MB
* 50K texts, <2 rep n-grams dropped
** 10.8%, 34ms - load("nFreq-50000-2-2-2-2-2.Rda") 1.0MB
* 50K texts, <10/6/4/3/2 rep n-grams dropped
** 11.8%, 27ms - load("nFreq-50000-10-6-4-3-2.Rda") 0.7MB
* 100K texts
** 12.3%, 37ms - load("nFreq-100000-2-2-2-2-2.Rda") 1.2MB
* 200K texts, hyphens intact in 1-grams, punct intact in 2+ grams
** 15.3%, 395ms - load("nFreq-200000-10-2-2-2-2-new.Rda")  11.7 MB
* w/ profanity removed
** 15.3%, 395ms - load("nFreq-200000-10-2-2-2-2.Rda")  11.6 MB
* further optimization, <3 rep 3-grams dropped
** 15.5%, 353ms - load("nFreq-200000-10-2-3-2-2.Rda")  10.2 MB
* further optimization, <3 rep 2 and 3-grams dropped
** 15.2%, 287ms - load("nFreq-200000-10-3-3-2-2.Rda")  9.1 MB



>
> save(nf, file="data/nFreq-200000-10-2-2-2-2-new.Rda")
> dim(nf.1)
[1] 19050     2
> dim(nf.2)
[1] 340191      2
> dim(nf.3)
[1] 319739      2
> dim(nf.4)
[1] 135801      2
> dim(nf.5)
[1] 41407     2
>


10-3-3-2-2

> dim(nf$f1)
[1] 18936     2
> dim(nf$f2)
[1] 199966      2
> dim(nf$f3)
[1] 150489      2
> dim(nf$f4)
[1] 139984      2
> dim(nf$f5)
[1] 43024     2
>
>
