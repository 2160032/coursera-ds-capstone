---
title: "Data Science Capstone Project"
author: patrick charles
date: "`r Sys.Date()`"
output:  
    html_document:
        keep_md: true
---

## Text Prediction (Exploratory Analysis and Prediction)

## Summary

A model and algorithms for text prediction are constructed. This is the final capstone project for the Johns Hopkins data science specialization certification series. 

In this dynamic document, the body of sample texts is loaded, exploratory analysis performed and a model and algorithm for word prediction built.

The [Capstone Dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) sample texts include content captured from blogs, new sources and twitter.

```{r global_options, include=FALSE}
  knitr::opts_chunk$set(fig.width=12, fig.height=8)

  local({
    r <- getOption("repos")
    r["CRAN"] <- "http://cran.cnr.berkeley.edu/"
    options(repos = r)
   })
```

```{r prereqs, message=FALSE, warning=FALSE, echo=FALSE}
## Prerequisites
  if(!require(tm)) install.packages("tm", dep=T)
  library(tm)
  if(!require(SnowballC)) install.packages("SnowballC", dep=T)
  library(SnowballC)
  if(!require(Rgraphviz)) {
    source("http://bioconductor.org/biocLite.R")
    biocLite("Rgraphviz")
  }
  if(!require(qdap)) install.packages("qdap", dep=T)
  library(qdap)
  if(!require(RWeka)) install.packages("RWeka", dep=T)
  library(RWeka)
  
  library(Rgraphviz)
  library(ggplot2)
  library(wordcloud)
  library(dplyr)
  library(slam)

  # custom functions for text prediction
  source("textPrediction.R")
```

## Load and Examine the Sample Texts

### Documents

The English-language content is used for the analysis.

```{r corpus.find}
  # view the English sample text source documents
  cpath <- file.path(".", "data", "final", "en_US")
  csize <- length(dir(cpath))
  dir(cpath)
```

```{r corpus.wc, echo=FALSE}
  wc <- function(ctree, corpus) {
    unlist(strsplit(sub("^ +", "",
      system(paste("wc ", ctree, corpus, sep=""),
      intern=TRUE)), split=" +"))
  }

  ctree <- "data/final/en_US/en_US."
  wc.blogs <- wc(ctree, "blogs.txt")
  wc.news <- wc(ctree, "news.txt")
  wc.twitter <- wc(ctree, "twitter.txt")

  wc.total.m <- round((as.numeric(wc.blogs[2]) +
                       as.numeric(wc.news[2]) +
                       as.numeric(wc.twitter[2]))/1000/1000, digits=0)

```
There are `r csize` documents in the English text samples.

* __blogs__ contains `r wc.blogs[1]` lines, `r wc.blogs[2]` words, and `r wc.blogs[3]` characters.
* __twitter__ contains `r wc.twitter[1]` lines, `r wc.twitter[2]` words, and `r wc.twitter[3]` characters.
* __news__ contains `r wc.news[1]` lines, `r wc.news[2]` words, and `r wc.news[3]` characters.

### Load Full Corpus of Texts

```{r corpus.load, cache=TRUE, echo=FALSE}

  blogs <- readLines("data/final/en_US/en_US.blogs.txt", skipNul=TRUE)
  twitter <- readLines("data/final/en_US/en_US.twitter.txt", skipNul=TRUE)
  news <- readLines("data/final/en_US/en_US.news.txt", skipNul=TRUE)

  texts.full <- c(blogs, news, twitter)

  name <- c("blog", "twitter", "news", "all")
  bytes <- c(object.size(blogs), object.size(twitter), object.size(news), object.size(texts.full))
  lines <- c(length(blogs), length(twitter), length(news), length(texts.full))
  corpus.info <- data.frame(name, bytes, lines)

  rm(blogs, twitter, news) # remove separate stores for each type
```

### Subset into a Training and Testing Corpus

```{r corpus.sub.load, cache=FALSE, echo=FALSE}
  texts.training <- sample(texts.full, 10000, replace=FALSE)
#  texts.training <- sample(texts.full, 500000, replace=FALSE)
  texts.testing <- sample(texts.full, 100, replace=FALSE)
```

## Cleaning and Transformation
```{r corpus.clean}
  # for generating predictive corpus, leave punctuation. tm/dtm uses.
  filtered <- createCleanCorpus(texts.training, remove.punct=FALSE)
  # for generating test text, remove punctuation
  filtered.test <- createCleanCorpus(texts.testing, remove.punct=TRUE)

  filtered
  filtered.test
```  


## Exploratory Analysis

A document-term matrix is created from the samples for the purpose of
analyzing word frequencies and characteristics.

### Most Frequently Occurring Terms

```{r explore.terms.freq}
  fthreshold <- 20 # frequency list entry threshold
  # minfreq <- 3 # too large. produces dim: 500000x229481 > 4503599627370496
  minfreq <- 10 # minimum required doc frequency for dtm
  dtm.1 <- DocumentTermMatrix(filtered, control=list(minDocFreq=minfreq))
  freq.1 <- sort(colSums(as.matrix(dtm.1)), decreasing=TRUE)
  wf.1 <- data.frame(word=names(freq.1), freq=freq.1)
  
  findFreqTerms(dtm.1, lowfreq=wf.1$freq[fthreshold])
 
  # plot frequencies
  ggplot(subset(wf.1, freq>wf.1$freq[fthreshold]),
    aes(reorder(word, freq), freq)) +
    geom_bar(stat="identity") + 
    theme(axis.text.x=element_text(angle=45, hjust=1)) +
    ggtitle("Most Common Words") + xlab("Word") + ylab("Frequency") 
```

### Least Frequently Occurring Terms

```{r explore.terms.least}
  head(findFreqTerms(dtm.1, highfreq=1), 10)
```

### Wordcloud

The wordcloud is a graphical visualization of word occurrence where
size is scaled by frequency.

```{r explore.terms.wc}
  set.seed(482)
  wordcloud(names(freq.1), freq.1, min.freq=40, max.words=100,
    colors=brewer.pal(8, "Dark2"), rot.per=0.35, scale=c(5, 0.5))
```

### Word Length Frequency

A histogram of number of letters by word frequency illustrates
the distribution of word lengths and highlights the average word length.

```{r explore.terms.wfreq, echo=FALSE}
  words <- dtm.1 %>%
    as.matrix %>%
    colnames %>%
    (function(x) x[nchar(x) < 20])
  words.lengths <- data.frame(length=nchar(words))
  ggplot(words.lengths, aes(x=length)) +
    geom_histogram(binwidth=1) + 
    geom_vline(xintercept=mean(nchar(words)),
              color="red", size=1, alpha=0.5) + 
    labs(x="Letters", y="Words")
```

The average length word in the sample texts has `r round(mean(nchar(words)), digits=0)` characters.


### N-grams

n-grams are extracted to characterize the frequency of multi-word
clusters.

```{r explore.ngrams, cache=TRUE}
  # sentence delimiters; prevent clustering across sentence boundaries
  delimiters <- " \\t\\r\\n.!?,;\"()"

  # n-gram tokenizers
  BigramTokenizer <-
    function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
  TrigramTokenizer <-
    function(x, n) NGramTokenizer(x, Weka_control(min=3, max=3))
  QuadgramTokenizer <-
    function(x, n) NGramTokenizer(x, Weka_control(min=4, max=4))
  PentagramTokenizer <-
    function(x, n) NGramTokenizer(x, Weka_control(min=5, max=5))
 
  gthreshold <- 15 # threshold for number of gram matches to display
  options(mc.cores=1) # limit cores to prevent rweka processing problems
  
  dtm.2 <-
    DocumentTermMatrix(filtered, control=list(tokenize=BigramTokenizer))
  freq.2 <- sort(colSums(as.matrix(dtm.2)), decreasing=TRUE)
  wf.2 <- data.frame(word=names(freq.2), freq=freq.2)
  plotGram(gthreshold, freq.2, wf.2, "Bigram")

  dtm.3 <-
    DocumentTermMatrix(filtered, control=list(tokenize=TrigramTokenizer))
  freq.3 <- sort(colSums(as.matrix(dtm.3)), decreasing=TRUE)
  wf.3 <- data.frame(word=names(freq.3), freq=freq.3)
  plotGram(gthreshold, freq.3, wf.3, "Trigram")

  dtm.4 <-
    DocumentTermMatrix(filtered, control=list(tokenize=QuadgramTokenizer))
  freq.4 <- sort(colSums(as.matrix(dtm.4)), decreasing=TRUE)
  wf.4 <- data.frame(word=names(freq.4), freq=freq.4)
  plotGram(gthreshold, freq.4, wf.4, "Quadgram")

  dtm.5 <-
    DocumentTermMatrix(filtered, control=list(tokenize=PentagramTokenizer))
  freq.5 <- sort(colSums(as.matrix(dtm.5)), decreasing=TRUE)
  wf.5 <- data.frame(word=names(freq.5), freq=freq.5)
  plotGram(gthreshold, freq.5, wf.5, "Quadgram")
```

### N-Gram Distribution

```{r optimization.count}
  # return the number of entries with frequency exceeding count
  countAboveFrequency <- function(wf, count) {
    dim(wf[wf$freq > count, ])[1]
  }
```

#### Total Count (Unique)
  * pentagrams: **`r countAboveFrequency(wf.5, 0)`**
  * quadgrams: **`r countAboveFrequency(wf.4, 0)`**
  * trigrams: **`r countAboveFrequency(wf.3, 0)`**
  * bigrams: **`r countAboveFrequency(wf.2, 0)`**
  * words: **`r countAboveFrequency(wf.1, 0)`**

####  Occurring More Than Once
  * pentagrams: **`r countAboveFrequency(wf.5, 1)`**
  * quadgrams: **`r countAboveFrequency(wf.4, 1)`**
  * trigrams: **`r countAboveFrequency(wf.3, 1)`**
  * bigrams: **`r countAboveFrequency(wf.2, 1)`**
  * words: **`r countAboveFrequency(wf.1, 1)`**

#### Occurring More Than Twice
  * pentagrams: **`r countAboveFrequency(wf.5, 2)`**
  * quadgrams: **`r countAboveFrequency(wf.4, 2)`**
  * trigrams: **`r countAboveFrequency(wf.3, 2)`**
  * bigrams: **`r countAboveFrequency(wf.2, 2)`**
  * words: **`r countAboveFrequency(wf.1, 2)`**


```{r optimization.prune}
  # prune the word frequencies, removing entries w/ frequency < count
  prune <- function(wf, count) {
    wf[wf$freq > count, ]
  }

  prune.threshold = 3

  wf.5.pruned <- prune(wf.5, prune.threshold)
  wf.4.pruned <- prune(wf.4, prune.threshold)
  wf.3.pruned <- prune(wf.3, prune.threshold)
  wf.2.pruned <- prune(wf.2, prune.threshold)
  wf.1.pruned <- prune(wf.1, prune.threshold)


#!!!
  # dtm.1 <- DocumentTermMatrix(filtered, control=list(bounds=list(global=c(10, Inf))))
```


## Prediction

### Prediction Functions/Algorithm

```{r predict.prepare}
  r <- 10 # frequency span for last-resort randomization
  nf <- list("f1"=wf.1, "f2"=wf.2, "f3"=wf.3, "f4"=wf.4, "f5"=wf.5, "r"=r)

  # this file is used by the text-prediction shiny application
  # as it encapsulates everything needed for textPrediction::predictNext()
#  save(nf, file="nFreq.Rda") # save the ngram frequencies to disk
```

### Prediction Tests (Unit Tests)

```{r predict.test}

  # 4-gram matches
  predictNext("could be a", nf)
  predictNext("i have to say thanks for the", nf)
  predictNext("a few years", nf)
  predictNext("the first time", nf)
  predictNext("i am so", nf)
  predictNext("ejefiei i am so", nf)
  
  # 3-gram matches
  predictNext("be a", nf)
  predictNext("can not", nf)
  predictNext("no matter", nf)
  predictNext("jefjieie no matter", nf)

  # 2-gram matches
  predictNext("a", nf)
  predictNext("will", nf)
  predictNext("could", nf)
  predictNext("ejfejke could", nf)

  # non-matches
  predictNext("jkefjiee", nf)
```

### Accuracy Tests

Random substrings are extracted from the testing text set.
The last word is excluded and the prediction model called on
the string. The actual last word is then compared with the predicted
last word, to guage the accuracy of the model.

```{r predict.accuracy}
  # extract a random substring of the provided text
  #   text - a string of characters containing words
  #
  # returns both a substring and the actual next word, for prediction testing
  randomSubstring <- function(text) {
    # convert characters to a vector
    wv <- unlist(strsplit(text, " "))
    wv.start <- as.integer(runif(1, 1, length(wv) - 1))
    wv.length <- as.integer(runif(1, 1, length(wv) - wv.start + 1))
    wv.sub <- paste(wv[wv.start:(wv.start + wv.length - 1)], collapse=" ")
    wv.next <- paste(wv[(wv.start + wv.length):(wv.start + wv.length)], collapse=" ")

    list("sub"=wv.sub, "nxt"=wv.next)
  }

  success <- 0
  invalid <- 0
  for(i in 1:length(filtered.test)) {
    testText <- filtered.test[[i]]$content

    # exclude testing texts with only a single word (e.g. nothing to predict!)
    if(wordCount(testText) > 1) {
      ts <- randomSubstring(testText)
      if(predictNext(ts$sub, nf)[1] == ts$nxt) success = success + 1
    }
    else {
      invalid <- invalid + 1 # count of invalid tests
    }
  }

  accuracy <- success * 100 / (length(filtered.test) - invalid)
```

# accuracies
#  ~7-8% - load("nFreq-10000-1-1-1-1-1.Rda")
#  ~7-8% - load("nFreq-20000-1-1-1-1-1.Rda")        9 MB
#     9% - load("nFreq-50000-2-2-2-2-2.Rda")        1 MB
# 14-18% - load("nFreq-200000-10-2-2-2-2-2.Rda")  11.7 MB

**`r length(filtered.test)`** strings were set aside in a test dataset.

**`r length(filtered.test) - invalid`** test strings were valid / testable.

Substrings were randomly selected from the test data and the model 
successfully predicted the actual last word of the substring
**`r success`** of **`r 100 - invalid`** times.

The measured accuracy of the model is **`r round(accuracy, digits=2)`%**.
  

## Next Steps

* apply same filters to input as output (lowercase, contractions)

* increase size of ngrams stored

* prune the ngrams, see !!! above

* optimize

* remove code from display




```{r other}
#  tdm <-
#    TermDocumentMatrix(filtered, control=list(tokenize =
#      BigramTokenizer))
#  inspect(tdm)

  customPunctuation <- function (x, preserve_intra_word_dashes = FALSE) {
    rpunct <- function(x) {
      x <- gsub("'", "\002", x)
      x <- gsub("[[:punct:]]+", "", x)
      gsub("\002", "'", x, fixed = TRUE)
    }
    if(preserve_intra_word_dashes) {
      x <- gsub("(\\w)-(\\w)", "\\1\001\\2", x)
      x <- rpunct(x)
      gsub("\001", "-", x, fixed = TRUE)
    } else {
      rpunct(x)
    }
  }
#  filtered <- tm_map(filtered, customPunctuation, preserve_intra_word_dash=T)


  # emoticons (https://en.wikipedia.org/wiki/List_of_emoticons)
  emoticons <- c(":-)", ":)", ":D", ":o)", ":]",
  ":3", ":c)", ":>", "=]", "8)", "=)", ":}", ":^)", ":-D", "8-D", "8D", "x-D", "xD", "X-D", "XD", "=-D", "=D", "=-3",
  "=3", "B^D", ":-))", ">:[", ":-(", ":(", ":-c", ":c", ":-<", ":<", ":-[", ":[", ":{", ";(", ":-||", ":@", ">:(", ":'-(", ":'(", ":'-)", ":')", "D:<", "D:", "D8", "D;",
  "D=", "DX", "v.v", "D-':", ">:O", ":-O", ":O", ":-o", ":o", "8-0", "O_O", "o-o", "O_o", "o_O", "o_o", "O-O", ";D", ">:P", ":-P", ":P", "X-P", "x-p", "xp", "XP", ":-p", ":p", 
  "=p", ":-b", ":b", "d:", ":L", "=L", ":S", ":-X", ":X", ":-#", ":#", "O:-)", "0:-3", "0:3", "0:-)", "0:)", "0;^)", ":-J", "|-O", 
  "<3", "</3")

  # remove emoticons
#  filtered <- tm_map(filtered, removeWords, emoticons)

```